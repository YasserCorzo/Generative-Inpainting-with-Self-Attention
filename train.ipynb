{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "ip3dhVU9DtyQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39efa31c-1fec-4b9c-b01d-29d3a8140a27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "SKWRzrH_DtyU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "from layers import GatedConv, ResizeGatedConv\n",
        "from torchsummary import summary\n",
        "import ssl\n",
        "\n",
        "ssl._create_default_https_context = ssl._create_unverified_context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "aknvPvPZDtyV"
      },
      "outputs": [],
      "source": [
        "def get_images(batch_size):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((32, 32)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    cifar10_train = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "    data_loader = torch.utils.data.DataLoader(cifar10_train, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    return data_loader\n",
        "\n",
        "\n",
        "def get_mask(image_size, square_size):\n",
        "    mask = np.zeros(image_size, dtype=np.uint8)\n",
        "    start = (image_size[0] - square_size) // 2\n",
        "    end = start + square_size\n",
        "    mask[start:end, start:end] = 1\n",
        "    mask = np.asarray(mask, np.float32)\n",
        "    mask = torch.from_numpy(mask)\n",
        "    return mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "D9upd6KsDtyV"
      },
      "outputs": [],
      "source": [
        "class FreeFormImageInpaint(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(FreeFormImageInpaint, self).__init__()\n",
        "\n",
        "        self.coarse_network = nn.Sequential(\n",
        "            GatedConv(in_channels, out_channels=32, kernel_size=5, stride=1, padding=2), # batch_size x 32 x 256 x 256\n",
        "            GatedConv(in_channels=32, out_channels=64, kernel_size=3, stride=2, padding=1), # batch_size x 64 x 128 x 128\n",
        "            GatedConv(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1), # batch_size x 64 x 128 x 128\n",
        "            GatedConv(in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=1), # batch_size x 128 x 64 x 64\n",
        "            GatedConv(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1), # batch_size x 128 x 64 x 64\n",
        "            GatedConv(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1), # batch_size x 128 x 64 x 64\n",
        "            GatedConv(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=2, dilation=2), # batch_size x 128 x 64 x 64\n",
        "            GatedConv(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=4, dilation=4), # batch_size x 128 x 64 x 64\n",
        "            GatedConv(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=8, dilation=8), # batch_size x 128 x 64 x 64\n",
        "            GatedConv(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=16, dilation=16), # batch_size x 128 x 64 x 64\n",
        "            GatedConv(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1), # batch_size x 128 x 64 x 64\n",
        "            GatedConv(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1), # batch_size x 128 x 64 x 64\n",
        "            ResizeGatedConv(in_channels=128, out_channels=64, padding=1, scale_factor=2), # batch_size x 64 x 128 x 128\n",
        "            GatedConv(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1), # batch_size x 64 x 128 x 128\n",
        "            ResizeGatedConv(in_channels=64, out_channels=32, padding=1, scale_factor=2), # batch_size x 32 x 256 x 256\n",
        "            GatedConv(in_channels=32, out_channels=16, kernel_size=3, stride=1, padding=1), # batch_size x 16 x 256 x 256\n",
        "            GatedConv(in_channels=16, out_channels=3, kernel_size=3, stride=1, padding=1, feature_act=None) # batch_size x 3 x 256 x 256\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x, masks):\n",
        "        \"\"\"\n",
        "        dim of x: batch_size x 256 x 256 x channels\n",
        "        dim of mask: batch_size x 256 x 256\n",
        "        \"\"\"\n",
        "        #print(\"shape of images (B x C x H x W):\", x.shape)\n",
        "        #print(\"shape of masks (B x H x W):\", masks.shape)\n",
        "        # TODO: normalize images and pair images with corresponding masks as input\n",
        "        # input will contain masked images\n",
        "        #x = x.permute(0, 3, 1, 2) # batch_size x channels x 256 x 256\n",
        "        masks = masks.unsqueeze(1) # batch_size x 1 x 256 x 256\n",
        "        masked_imgs = x * (1 - masks)\n",
        "        #print(masks.shape)\n",
        "        #print(masked_imgs.shape)\n",
        "        input = torch.cat([masked_imgs, masks], dim=1) # batch_size x (channels + 1) x 256 x 256\n",
        "        #print(\"shape of input into coarse network:\", input.shape)\n",
        "\n",
        "        # coarse network\n",
        "        coarse_out = self.coarse_network(input)\n",
        "        # clip output so values are between -1 and 1\n",
        "        coarse_clip = torch.clamp(coarse_out, -1, 1)\n",
        "\n",
        "        return coarse_clip\n",
        "\n",
        "    def loss_function(self, x_hat, x, masks, alpha):\n",
        "        '''\n",
        "        dim of x_hat & x: batch_size x 3 x 256 x 256\n",
        "        dim of masks: batch_size x 256 x 256\n",
        "        '''\n",
        "\n",
        "        # TODO: convert x/x_hat to just masked and unmasked portion\n",
        "        #print(\"shape of x:\", x.shape)\n",
        "        #print(\"shape of xhat:\", x_hat.shape)\n",
        "        masks = masks.unsqueeze(1) # batch_size x 1 x 256 x 256\n",
        "        #print(\"shape of masks:\", masks.shape)\n",
        "        unmasked = x * masks\n",
        "        unmasked_hat = x_hat * masks\n",
        "        masked = x * (1 - masks)\n",
        "        masked_hat = x_hat * (1 - masks)\n",
        "\n",
        "        mask_bit_ratio = torch.mean(torch.mean(masks, -1), -1) #take the ratio of masked to unmasked bits\n",
        "        mask_bit_ratio = mask_bit_ratio.unsqueeze(-1)\n",
        "        mask_bit_ratio = mask_bit_ratio.unsqueeze(-1)\n",
        "        #print(mask_bit_ratio.shape)\n",
        "        bit_mask_ratio = torch.mean(torch.mean(1-masks, -1), -1) #take the ratio of unmasked to masked bits\n",
        "        bit_mask_ratio = bit_mask_ratio.unsqueeze(-1)\n",
        "        bit_mask_ratio = bit_mask_ratio.unsqueeze(-1)\n",
        "        masked_loss = alpha * torch.mean(torch.abs(masked - masked_hat) / mask_bit_ratio)\n",
        "        unmasked_loss = alpha * torch.mean(torch.abs(unmasked - unmasked_hat) / bit_mask_ratio)\n",
        "        loss = masked_loss + unmasked_loss\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6zOD204DtyW",
        "outputId": "41d0163e-206b-4fd1-d44e-58afe3fd3a7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "jSb524-eDtyX"
      },
      "outputs": [],
      "source": [
        "n_epochs = 20\n",
        "alpha = 0.02\n",
        "learning_rate = 0.001\n",
        "batch_size = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3i2mPa2LDtyX",
        "outputId": "583b15dd-19ca-4a02-8baa-e2239ddf610e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "torch.Size([256, 32, 32])\n"
          ]
        }
      ],
      "source": [
        "image_loader = get_images(batch_size)\n",
        "image_size = (32, 32)\n",
        "square_size = 10\n",
        "binary_mask = get_mask(image_size, square_size)\n",
        "# reshape binary mask to add batch_size dimension\n",
        "binary_mask = binary_mask.unsqueeze(0)\n",
        "binary_mask = binary_mask.expand(batch_size, -1, -1)\n",
        "binary_mask = binary_mask.to(device)\n",
        "\n",
        "print(binary_mask.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Zbz9FBFHDtyY",
        "outputId": "94f11163-e91c-439d-8f15-c0d14b662d4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]           3,232\n",
            "            Conv2d-2           [-1, 32, 32, 32]           3,232\n",
            "           Sigmoid-3           [-1, 32, 32, 32]               0\n",
            "         LeakyReLU-4           [-1, 32, 32, 32]               0\n",
            "         LeakyReLU-5           [-1, 32, 32, 32]               0\n",
            "         LeakyReLU-6           [-1, 32, 32, 32]               0\n",
            "         LeakyReLU-7           [-1, 32, 32, 32]               0\n",
            "         LeakyReLU-8           [-1, 32, 32, 32]               0\n",
            "         LeakyReLU-9           [-1, 32, 32, 32]               0\n",
            "        LeakyReLU-10           [-1, 32, 32, 32]               0\n",
            "        LeakyReLU-11           [-1, 32, 32, 32]               0\n",
            "        LeakyReLU-12           [-1, 32, 32, 32]               0\n",
            "        LeakyReLU-13           [-1, 32, 32, 32]               0\n",
            "        LeakyReLU-14           [-1, 32, 32, 32]               0\n",
            "        LeakyReLU-15           [-1, 32, 32, 32]               0\n",
            "        LeakyReLU-16           [-1, 32, 32, 32]               0\n",
            "        LeakyReLU-17           [-1, 32, 32, 32]               0\n",
            "        LeakyReLU-18           [-1, 32, 32, 32]               0\n",
            "        LeakyReLU-19           [-1, 32, 32, 32]               0\n",
            "      BatchNorm2d-20           [-1, 32, 32, 32]              64\n",
            "        GatedConv-21           [-1, 32, 32, 32]               0\n",
            "           Conv2d-22           [-1, 64, 16, 16]          18,496\n",
            "           Conv2d-23           [-1, 64, 16, 16]          18,496\n",
            "          Sigmoid-24           [-1, 64, 16, 16]               0\n",
            "        LeakyReLU-25           [-1, 64, 16, 16]               0\n",
            "        LeakyReLU-26           [-1, 64, 16, 16]               0\n",
            "        LeakyReLU-27           [-1, 64, 16, 16]               0\n",
            "        LeakyReLU-28           [-1, 64, 16, 16]               0\n",
            "        LeakyReLU-29           [-1, 64, 16, 16]               0\n",
            "        LeakyReLU-30           [-1, 64, 16, 16]               0\n",
            "        LeakyReLU-31           [-1, 64, 16, 16]               0\n",
            "        LeakyReLU-32           [-1, 64, 16, 16]               0\n",
            "        LeakyReLU-33           [-1, 64, 16, 16]               0\n",
            "        LeakyReLU-34           [-1, 64, 16, 16]               0\n",
            "        LeakyReLU-35           [-1, 64, 16, 16]               0\n",
            "        LeakyReLU-36           [-1, 64, 16, 16]               0\n",
            "        LeakyReLU-37           [-1, 64, 16, 16]               0\n",
            "        LeakyReLU-38           [-1, 64, 16, 16]               0\n",
            "        LeakyReLU-39           [-1, 64, 16, 16]               0\n",
            "        LeakyReLU-40           [-1, 64, 16, 16]               0\n",
            "      BatchNorm2d-41           [-1, 64, 16, 16]             128\n",
            "        GatedConv-42           [-1, 64, 16, 16]               0\n",
            "           Conv2d-43           [-1, 64, 16, 16]          36,928\n",
            "           Conv2d-44           [-1, 64, 16, 16]          36,928\n",
            "          Sigmoid-45           [-1, 64, 16, 16]               0\n",
            "        LeakyReLU-46           [-1, 64, 16, 16]               0\n",
            "        LeakyReLU-47           [-1, 64, 16, 16]               0\n",
            "        LeakyReLU-48           [-1, 64, 16, 16]               0\n",
            "        LeakyReLU-49           [-1, 64, 16, 16]               0\n",
            "        LeakyReLU-50           [-1, 64, 16, 16]               0\n",
            "        LeakyReLU-51           [-1, 64, 16, 16]               0\n",
            "        LeakyReLU-52           [-1, 64, 16, 16]               0\n",
            "        LeakyReLU-53           [-1, 64, 16, 16]               0\n",
            "        LeakyReLU-54           [-1, 64, 16, 16]               0\n",
            "        LeakyReLU-55           [-1, 64, 16, 16]               0\n",
            "        LeakyReLU-56           [-1, 64, 16, 16]               0\n",
            "        LeakyReLU-57           [-1, 64, 16, 16]               0\n",
            "        LeakyReLU-58           [-1, 64, 16, 16]               0\n",
            "        LeakyReLU-59           [-1, 64, 16, 16]               0\n",
            "        LeakyReLU-60           [-1, 64, 16, 16]               0\n",
            "        LeakyReLU-61           [-1, 64, 16, 16]               0\n",
            "      BatchNorm2d-62           [-1, 64, 16, 16]             128\n",
            "        GatedConv-63           [-1, 64, 16, 16]               0\n",
            "           Conv2d-64            [-1, 128, 8, 8]          73,856\n",
            "           Conv2d-65            [-1, 128, 8, 8]          73,856\n",
            "          Sigmoid-66            [-1, 128, 8, 8]               0\n",
            "        LeakyReLU-67            [-1, 128, 8, 8]               0\n",
            "        LeakyReLU-68            [-1, 128, 8, 8]               0\n",
            "        LeakyReLU-69            [-1, 128, 8, 8]               0\n",
            "        LeakyReLU-70            [-1, 128, 8, 8]               0\n",
            "        LeakyReLU-71            [-1, 128, 8, 8]               0\n",
            "        LeakyReLU-72            [-1, 128, 8, 8]               0\n",
            "        LeakyReLU-73            [-1, 128, 8, 8]               0\n",
            "        LeakyReLU-74            [-1, 128, 8, 8]               0\n",
            "        LeakyReLU-75            [-1, 128, 8, 8]               0\n",
            "        LeakyReLU-76            [-1, 128, 8, 8]               0\n",
            "        LeakyReLU-77            [-1, 128, 8, 8]               0\n",
            "        LeakyReLU-78            [-1, 128, 8, 8]               0\n",
            "        LeakyReLU-79            [-1, 128, 8, 8]               0\n",
            "        LeakyReLU-80            [-1, 128, 8, 8]               0\n",
            "        LeakyReLU-81            [-1, 128, 8, 8]               0\n",
            "        LeakyReLU-82            [-1, 128, 8, 8]               0\n",
            "      BatchNorm2d-83            [-1, 128, 8, 8]             256\n",
            "        GatedConv-84            [-1, 128, 8, 8]               0\n",
            "           Conv2d-85            [-1, 128, 8, 8]         147,584\n",
            "           Conv2d-86            [-1, 128, 8, 8]         147,584\n",
            "          Sigmoid-87            [-1, 128, 8, 8]               0\n",
            "        LeakyReLU-88            [-1, 128, 8, 8]               0\n",
            "        LeakyReLU-89            [-1, 128, 8, 8]               0\n",
            "        LeakyReLU-90            [-1, 128, 8, 8]               0\n",
            "        LeakyReLU-91            [-1, 128, 8, 8]               0\n",
            "        LeakyReLU-92            [-1, 128, 8, 8]               0\n",
            "        LeakyReLU-93            [-1, 128, 8, 8]               0\n",
            "        LeakyReLU-94            [-1, 128, 8, 8]               0\n",
            "        LeakyReLU-95            [-1, 128, 8, 8]               0\n",
            "        LeakyReLU-96            [-1, 128, 8, 8]               0\n",
            "        LeakyReLU-97            [-1, 128, 8, 8]               0\n",
            "        LeakyReLU-98            [-1, 128, 8, 8]               0\n",
            "        LeakyReLU-99            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-100            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-101            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-102            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-103            [-1, 128, 8, 8]               0\n",
            "     BatchNorm2d-104            [-1, 128, 8, 8]             256\n",
            "       GatedConv-105            [-1, 128, 8, 8]               0\n",
            "          Conv2d-106            [-1, 128, 8, 8]         147,584\n",
            "          Conv2d-107            [-1, 128, 8, 8]         147,584\n",
            "         Sigmoid-108            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-109            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-110            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-111            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-112            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-113            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-114            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-115            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-116            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-117            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-118            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-119            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-120            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-121            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-122            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-123            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-124            [-1, 128, 8, 8]               0\n",
            "     BatchNorm2d-125            [-1, 128, 8, 8]             256\n",
            "       GatedConv-126            [-1, 128, 8, 8]               0\n",
            "          Conv2d-127            [-1, 128, 8, 8]         147,584\n",
            "          Conv2d-128            [-1, 128, 8, 8]         147,584\n",
            "         Sigmoid-129            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-130            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-131            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-132            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-133            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-134            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-135            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-136            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-137            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-138            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-139            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-140            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-141            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-142            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-143            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-144            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-145            [-1, 128, 8, 8]               0\n",
            "     BatchNorm2d-146            [-1, 128, 8, 8]             256\n",
            "       GatedConv-147            [-1, 128, 8, 8]               0\n",
            "          Conv2d-148            [-1, 128, 8, 8]         147,584\n",
            "          Conv2d-149            [-1, 128, 8, 8]         147,584\n",
            "         Sigmoid-150            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-151            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-152            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-153            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-154            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-155            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-156            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-157            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-158            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-159            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-160            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-161            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-162            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-163            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-164            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-165            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-166            [-1, 128, 8, 8]               0\n",
            "     BatchNorm2d-167            [-1, 128, 8, 8]             256\n",
            "       GatedConv-168            [-1, 128, 8, 8]               0\n",
            "          Conv2d-169            [-1, 128, 8, 8]         147,584\n",
            "          Conv2d-170            [-1, 128, 8, 8]         147,584\n",
            "         Sigmoid-171            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-172            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-173            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-174            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-175            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-176            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-177            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-178            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-179            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-180            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-181            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-182            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-183            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-184            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-185            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-186            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-187            [-1, 128, 8, 8]               0\n",
            "     BatchNorm2d-188            [-1, 128, 8, 8]             256\n",
            "       GatedConv-189            [-1, 128, 8, 8]               0\n",
            "          Conv2d-190            [-1, 128, 8, 8]         147,584\n",
            "          Conv2d-191            [-1, 128, 8, 8]         147,584\n",
            "         Sigmoid-192            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-193            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-194            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-195            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-196            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-197            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-198            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-199            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-200            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-201            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-202            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-203            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-204            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-205            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-206            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-207            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-208            [-1, 128, 8, 8]               0\n",
            "     BatchNorm2d-209            [-1, 128, 8, 8]             256\n",
            "       GatedConv-210            [-1, 128, 8, 8]               0\n",
            "          Conv2d-211            [-1, 128, 8, 8]         147,584\n",
            "          Conv2d-212            [-1, 128, 8, 8]         147,584\n",
            "         Sigmoid-213            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-214            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-215            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-216            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-217            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-218            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-219            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-220            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-221            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-222            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-223            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-224            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-225            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-226            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-227            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-228            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-229            [-1, 128, 8, 8]               0\n",
            "     BatchNorm2d-230            [-1, 128, 8, 8]             256\n",
            "       GatedConv-231            [-1, 128, 8, 8]               0\n",
            "          Conv2d-232            [-1, 128, 8, 8]         147,584\n",
            "          Conv2d-233            [-1, 128, 8, 8]         147,584\n",
            "         Sigmoid-234            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-235            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-236            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-237            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-238            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-239            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-240            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-241            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-242            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-243            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-244            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-245            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-246            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-247            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-248            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-249            [-1, 128, 8, 8]               0\n",
            "       LeakyReLU-250            [-1, 128, 8, 8]               0\n",
            "     BatchNorm2d-251            [-1, 128, 8, 8]             256\n",
            "       GatedConv-252            [-1, 128, 8, 8]               0\n",
            "          Conv2d-253           [-1, 64, 16, 16]          73,792\n",
            "          Conv2d-254           [-1, 64, 16, 16]          73,792\n",
            "         Sigmoid-255           [-1, 64, 16, 16]               0\n",
            "       LeakyReLU-256           [-1, 64, 16, 16]               0\n",
            "       LeakyReLU-257           [-1, 64, 16, 16]               0\n",
            "       LeakyReLU-258           [-1, 64, 16, 16]               0\n",
            "       LeakyReLU-259           [-1, 64, 16, 16]               0\n",
            "       LeakyReLU-260           [-1, 64, 16, 16]               0\n",
            "       LeakyReLU-261           [-1, 64, 16, 16]               0\n",
            "       LeakyReLU-262           [-1, 64, 16, 16]               0\n",
            "       LeakyReLU-263           [-1, 64, 16, 16]               0\n",
            "       LeakyReLU-264           [-1, 64, 16, 16]               0\n",
            "       LeakyReLU-265           [-1, 64, 16, 16]               0\n",
            "       LeakyReLU-266           [-1, 64, 16, 16]               0\n",
            "       LeakyReLU-267           [-1, 64, 16, 16]               0\n",
            "       LeakyReLU-268           [-1, 64, 16, 16]               0\n",
            "       LeakyReLU-269           [-1, 64, 16, 16]               0\n",
            "       LeakyReLU-270           [-1, 64, 16, 16]               0\n",
            "       LeakyReLU-271           [-1, 64, 16, 16]               0\n",
            "     BatchNorm2d-272           [-1, 64, 16, 16]             128\n",
            "       GatedConv-273           [-1, 64, 16, 16]               0\n",
            " ResizeGatedConv-274           [-1, 64, 16, 16]               0\n",
            "          Conv2d-275           [-1, 64, 16, 16]          36,928\n",
            "          Conv2d-276           [-1, 64, 16, 16]          36,928\n",
            "         Sigmoid-277           [-1, 64, 16, 16]               0\n",
            "       LeakyReLU-278           [-1, 64, 16, 16]               0\n",
            "       LeakyReLU-279           [-1, 64, 16, 16]               0\n",
            "       LeakyReLU-280           [-1, 64, 16, 16]               0\n",
            "       LeakyReLU-281           [-1, 64, 16, 16]               0\n",
            "       LeakyReLU-282           [-1, 64, 16, 16]               0\n",
            "       LeakyReLU-283           [-1, 64, 16, 16]               0\n",
            "       LeakyReLU-284           [-1, 64, 16, 16]               0\n",
            "       LeakyReLU-285           [-1, 64, 16, 16]               0\n",
            "       LeakyReLU-286           [-1, 64, 16, 16]               0\n",
            "       LeakyReLU-287           [-1, 64, 16, 16]               0\n",
            "       LeakyReLU-288           [-1, 64, 16, 16]               0\n",
            "       LeakyReLU-289           [-1, 64, 16, 16]               0\n",
            "       LeakyReLU-290           [-1, 64, 16, 16]               0\n",
            "       LeakyReLU-291           [-1, 64, 16, 16]               0\n",
            "       LeakyReLU-292           [-1, 64, 16, 16]               0\n",
            "       LeakyReLU-293           [-1, 64, 16, 16]               0\n",
            "     BatchNorm2d-294           [-1, 64, 16, 16]             128\n",
            "       GatedConv-295           [-1, 64, 16, 16]               0\n",
            "          Conv2d-296           [-1, 32, 32, 32]          18,464\n",
            "          Conv2d-297           [-1, 32, 32, 32]          18,464\n",
            "         Sigmoid-298           [-1, 32, 32, 32]               0\n",
            "       LeakyReLU-299           [-1, 32, 32, 32]               0\n",
            "       LeakyReLU-300           [-1, 32, 32, 32]               0\n",
            "       LeakyReLU-301           [-1, 32, 32, 32]               0\n",
            "       LeakyReLU-302           [-1, 32, 32, 32]               0\n",
            "       LeakyReLU-303           [-1, 32, 32, 32]               0\n",
            "       LeakyReLU-304           [-1, 32, 32, 32]               0\n",
            "       LeakyReLU-305           [-1, 32, 32, 32]               0\n",
            "       LeakyReLU-306           [-1, 32, 32, 32]               0\n",
            "       LeakyReLU-307           [-1, 32, 32, 32]               0\n",
            "       LeakyReLU-308           [-1, 32, 32, 32]               0\n",
            "       LeakyReLU-309           [-1, 32, 32, 32]               0\n",
            "       LeakyReLU-310           [-1, 32, 32, 32]               0\n",
            "       LeakyReLU-311           [-1, 32, 32, 32]               0\n",
            "       LeakyReLU-312           [-1, 32, 32, 32]               0\n",
            "       LeakyReLU-313           [-1, 32, 32, 32]               0\n",
            "       LeakyReLU-314           [-1, 32, 32, 32]               0\n",
            "     BatchNorm2d-315           [-1, 32, 32, 32]              64\n",
            "       GatedConv-316           [-1, 32, 32, 32]               0\n",
            " ResizeGatedConv-317           [-1, 32, 32, 32]               0\n",
            "          Conv2d-318           [-1, 16, 32, 32]           4,624\n",
            "          Conv2d-319           [-1, 16, 32, 32]           4,624\n",
            "         Sigmoid-320           [-1, 16, 32, 32]               0\n",
            "       LeakyReLU-321           [-1, 16, 32, 32]               0\n",
            "       LeakyReLU-322           [-1, 16, 32, 32]               0\n",
            "       LeakyReLU-323           [-1, 16, 32, 32]               0\n",
            "       LeakyReLU-324           [-1, 16, 32, 32]               0\n",
            "       LeakyReLU-325           [-1, 16, 32, 32]               0\n",
            "       LeakyReLU-326           [-1, 16, 32, 32]               0\n",
            "       LeakyReLU-327           [-1, 16, 32, 32]               0\n",
            "       LeakyReLU-328           [-1, 16, 32, 32]               0\n",
            "       LeakyReLU-329           [-1, 16, 32, 32]               0\n",
            "       LeakyReLU-330           [-1, 16, 32, 32]               0\n",
            "       LeakyReLU-331           [-1, 16, 32, 32]               0\n",
            "       LeakyReLU-332           [-1, 16, 32, 32]               0\n",
            "       LeakyReLU-333           [-1, 16, 32, 32]               0\n",
            "       LeakyReLU-334           [-1, 16, 32, 32]               0\n",
            "       LeakyReLU-335           [-1, 16, 32, 32]               0\n",
            "       LeakyReLU-336           [-1, 16, 32, 32]               0\n",
            "     BatchNorm2d-337           [-1, 16, 32, 32]              32\n",
            "       GatedConv-338           [-1, 16, 32, 32]               0\n",
            "          Conv2d-339            [-1, 3, 32, 32]             435\n",
            "          Conv2d-340            [-1, 3, 32, 32]             435\n",
            "         Sigmoid-341            [-1, 3, 32, 32]               0\n",
            "     BatchNorm2d-342            [-1, 3, 32, 32]               6\n",
            "       GatedConv-343            [-1, 3, 32, 32]               0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-48ff0ae1a6eb>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchsummary\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0minpaint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFreeFormImageInpaint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minpaint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchsummary/torchsummary.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;31m# assume 4 bytes/number (float on cuda).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0mtotal_input_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m4.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1024\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m     \u001b[0mtotal_output_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtotal_output\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m4.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1024\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# x2 for gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0mtotal_params_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m4.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1024\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mprod\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   3098\u001b[0m     \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3099\u001b[0m     \"\"\"\n\u001b[0;32m-> 3100\u001b[0;31m     return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out,\n\u001b[0m\u001b[1;32m   3101\u001b[0m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[1;32m   3102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part."
          ]
        }
      ],
      "source": [
        "from torchsummary import summary\n",
        "inpaint = FreeFormImageInpaint(in_channels=4).to(device)\n",
        "summary(inpaint, [(3, 32, 32), (32, 32)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "xXGM1PoODtyY"
      },
      "outputs": [],
      "source": [
        "inpaint = FreeFormImageInpaint(in_channels=4).to(device)\n",
        "optimizer = torch.optim.Adam(inpaint.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(image_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbB1owtiKgpB",
        "outputId": "812c1dd2-d38c-4baf-9d78-46b6a7bfc5be"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "196"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "til40Q3kDtyY",
        "outputId": "8bde2006-5514-4718-f4b1-406eb80e547e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 \tTraining Loss: 0.024470\n",
            "Epoch: 1 \tTraining Loss: 0.014601\n",
            "Epoch: 2 \tTraining Loss: 0.012335\n",
            "Epoch: 3 \tTraining Loss: 0.010665\n",
            "Epoch: 4 \tTraining Loss: 0.009768\n",
            "Epoch: 5 \tTraining Loss: 0.009265\n",
            "Epoch: 6 \tTraining Loss: 0.008589\n",
            "Epoch: 7 \tTraining Loss: 0.008274\n",
            "Epoch: 8 \tTraining Loss: 0.007982\n",
            "Epoch: 9 \tTraining Loss: 0.007741\n",
            "Epoch: 10 \tTraining Loss: 0.007501\n",
            "Epoch: 11 \tTraining Loss: 0.007263\n",
            "Epoch: 12 \tTraining Loss: 0.007117\n",
            "Epoch: 13 \tTraining Loss: 0.007022\n",
            "Epoch: 14 \tTraining Loss: 0.006948\n",
            "Epoch: 15 \tTraining Loss: 0.006930\n",
            "Epoch: 16 \tTraining Loss: 0.006889\n",
            "Epoch: 17 \tTraining Loss: 0.006776\n",
            "Epoch: 18 \tTraining Loss: 0.006793\n",
            "Epoch: 19 \tTraining Loss: 0.006731\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(n_epochs):\n",
        "    train_loss = 0\n",
        "    loss = 0\n",
        "    for batch_idx, (data, _) in enumerate(image_loader):\n",
        "        #print(batch_idx)\n",
        "        if batch_idx == 195:\n",
        "            break\n",
        "        data = data.to(device)\n",
        "        binary_mask = binary_mask.to(device)\n",
        "        # clear the gradients of all optimized variables\n",
        "        optimizer.zero_grad()\n",
        "        # model forward\n",
        "        x_hat = inpaint(data, binary_mask)\n",
        "        #print(\"shape of output:\", x_hat.shape)\n",
        "        # compute the loss\n",
        "        loss = inpaint.loss_function(x_hat, data, binary_mask, alpha)\n",
        "        # model backward\n",
        "        loss.backward()\n",
        "        # update the model paramters\n",
        "        optimizer.step()\n",
        "        # update running training loss\n",
        "        train_loss += loss\n",
        "    train_loss = train_loss/len(image_loader)\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch, train_loss))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QIZ93fVxQCk7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}